{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "Découpage des images Sentinel2, Attribution des labels au images, Enregistrement des images sur le disque, Calcul de la moyenne et écart-types des images\n",
    "\n",
    "* Romain Capocasale\n",
    "* IADeforestation\n",
    "* HES-SO MASTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romain.capocasa/romain_env/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.0-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shapefile\n",
    "import affine\n",
    "import statistics\n",
    "import geopandas as gpd\n",
    "import spacv\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry.point import Point\n",
    "from shapely import wkt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from IAdeforestation.preprocessing import *\n",
    "from IAdeforestation.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_PATH = 'labels'\n",
    "SHAPEFILES_PATHS = [os.path.join(LABELS_PATH, 'central_highlands_1_other', 'central_highlands_1_other.shp'),\n",
    " os.path.join(LABELS_PATH, 'central_highlands_2_test', 'central_highlands_2_test.shp'),\n",
    " os.path.join(LABELS_PATH, 'central_highlands_2_other', 'central_highlands_2_other.shp')]\n",
    "\n",
    "SHAPEFILE_ESPG=4326\n",
    "\n",
    "points = process_shapefile(SHAPEFILES_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create output image tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'spring_images_32'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "    for i in range(-1,33):\n",
    "        os.mkdir(os.path.join(OUTPUT_DIR, str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all Sentinel images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTINEL_IMAGES_PATH = 'SentinelImages'\n",
    "paths = os.listdir(SENTINEL_IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and export images on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ready\n",
      "b ready\n",
      "c ready\n",
      "d ready\n",
      "e ready\n"
     ]
    }
   ],
   "source": [
    "geo_paths = []\n",
    "\n",
    "TARGET_IMG_SIZE = 32 # Image size (width*height)\n",
    "NUMBER_OF_SPLIT = 343 # Number of split in big image, causion large image size > that TARGET_IMG_SIZE * NUMBER_OF_SPLIT\n",
    "i = 0\n",
    "\n",
    "for sentinel_image_path in paths:\n",
    "    raster_paths = get_raster_paths(os.path.join(SENTINEL_IMAGES_PATH, sentinel_image_path)) # Get each raster path\n",
    "    raster_dict = load_raster_img(raster_paths) # Load each raster\n",
    "    image_dict = resample_bands(raster_dict)\n",
    "\n",
    "    l = list(image_dict.values())\n",
    "    final_img = np.asarray(l)\n",
    "    split_export_img(final_img, \n",
    "                     raster_dict['B02'], \n",
    "                     points, \n",
    "                     img_prefix=str(i), \n",
    "                     crop_size=TARGET_IMG_SIZE,\n",
    "                     split_size=NUMBER_OF_SPLIT,\n",
    "                     export_folder=OUTPUT_DIR,\n",
    "                    geo_paths=geo_paths)\n",
    "    print(f\"{sentinel_image_path} ready\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DATASET_PATH = \"datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save datasets on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(pd.DataFrame(geo_paths, columns=['path', 'label', 'geometry'])).to_csv(os.path.join('datasets', 'start_all.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / test split stratified non-spatial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gpd.GeoDataFrame(pd.read_csv(os.path.join('datasets', 'start_all.csv')))\n",
    "all_data['geometry'] = all_data['geometry'].apply(wkt.loads)\n",
    "\n",
    "all_data.loc[all_data['label'] != 2, 'label'] = 1\n",
    "all_data.loc[all_data['label'] == 2, 'label'] = 0\n",
    "\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2)\n",
    "\n",
    "for train_index, test_index in sss.split(all_data['path'].to_numpy(), all_data['label']):\n",
    "    train_set = all_data.iloc[train_index]\n",
    "    test_set = all_data.iloc[test_index]\n",
    " \n",
    "\n",
    "train_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"strat_train.csv\"), index=False)\n",
    "test_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"strat_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "culture_list = [1,2,6,15,18,19,17] #  6 caoutchou, 25 poivre de cayenne, 16 cassava, 23 intercrop\n",
    "no_culture_list = [4,9,10, 24,27] # 11 other tree, 27 pines tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee vs other\n",
    "### 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gpd.GeoDataFrame(pd.read_csv(os.path.join('datasets', 'strat_all.csv')))\n",
    "all_data['geometry'] = all_data['geometry'].apply(wkt.loads)\n",
    "\n",
    "all_data.loc[all_data['label'] != 2, 'label'] = 0\n",
    "all_data.loc[all_data['label'] == 2, 'label'] = 1\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2)\n",
    "\n",
    "for train_index, test_index in sss.split(all_data['path'].to_numpy(), all_data['label']):\n",
    "    train_set = all_data.iloc[train_index]\n",
    "    test_set = all_data.iloc[test_index]\n",
    "    \n",
    "train_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"10strat_train.csv\"), index=False)\n",
    "test_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"10strat_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gpd.GeoDataFrame(pd.read_csv(os.path.join('datasets', 'strat_all_32.csv')))\n",
    "all_data['geometry'] = all_data['geometry'].apply(wkt.loads)\n",
    "\n",
    "all_data = all_data[all_data['label'].isin(culture_list + no_culture_list)]\n",
    "all_data.loc[train_data['label'] != 2, 'label'] = 1\n",
    "all_data.loc[train_data['label'] == 2, 'label'] = 0\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2)\n",
    "\n",
    "for train_index, test_index in sss.split(all_data['path'].to_numpy(), all_data['label']):\n",
    "    train_set = all_data.iloc[train_index]\n",
    "    test_set = all_data.iloc[test_index]\n",
    "    \n",
    "train_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"start_train_32.csv\"), index=False)\n",
    "test_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"start_test_32.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Culture vs no-culture\n",
    "### 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gpd.GeoDataFrame(pd.read_csv(os.path.join('datasets', 'start_all.csv')))\n",
    "all_data['geometry'] = all_data['geometry'].apply(wkt.loads)\n",
    "\n",
    "all_data = all_data[all_data['label'].isin(culture_list + no_culture_list)]\n",
    "all_data.loc[all_data['label'].isin(culture_list), 'label'] = 0\n",
    "all_data.loc[all_data['label'].isin(no_culture_list), 'label'] =1\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2)\n",
    "\n",
    "for train_index, test_index in sss.split(all_data['path'].to_numpy(), all_data['label']):\n",
    "    train_set = all_data.iloc[train_index]\n",
    "    test_set = all_data.iloc[test_index]\n",
    "    \n",
    "train_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"strat_train_culture.csv\"), index=False)\n",
    "test_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"strat_test_culture.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = gpd.GeoDataFrame(pd.read_csv(os.path.join('datasets', 'start_all.csv')))\n",
    "all_data['geometry'] = all_data['geometry'].apply(wkt.loads)\n",
    "\n",
    "all_data = all_data[all_data['label'].isin(culture_list + no_culture_list)]\n",
    "all_data.loc[all_data['label'].isin(culture_list), 'label'] = 0\n",
    "all_data.loc[all_data['label'].isin(no_culture_list), 'label'] =1\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2)\n",
    "\n",
    "for train_index, test_index in sss.split(all_data['path'].to_numpy(), all_data['label']):\n",
    "    train_set = all_data.iloc[train_index]\n",
    "    test_set = all_data.iloc[test_index]\n",
    "  \n",
    "train_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"strat_train_culture_32.csv\"), index=False)\n",
    "test_set.to_csv(os.path.join(SAVED_DATASET_PATH, \"strat_train_culture_32.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Mean/Std of images dataset\n",
    "The mean and standard deviation obtained following the execution of this code are used to normalize the images with the z-norm method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTINEL_IMAGES_PATH = 'SentinelImages'\n",
    "\n",
    "paths = os.listdir(SENTINEL_IMAGES_PATH)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "NB_SELECTED_VAL = 100000\n",
    "dict_val = dict.fromkeys(['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B09', 'B10', 'B11', 'B12', 'B8A'])\n",
    "\n",
    "for band in dict_val.keys():\n",
    "    dict_val[band] = np.array([])\n",
    "    \n",
    "for path in paths:\n",
    "    raster_paths = get_raster_paths(os.path.join(SENTINEL_IMAGES_PATH, path)) # Get each raster path\n",
    "    raster_dict = load_raster_img(raster_paths) # Load each raster\n",
    "    image_dict = resample_bands(raster_dict)\n",
    "\n",
    "    l = list(image_dict.values())\n",
    "    final_img = np.asarray(l)\n",
    "\n",
    "    for i, band_name in enumerate(dict_val.keys()):\n",
    "        dict_val[band_name] = np.append(dict_val[band_name], \n",
    "                                        np.random.choice(final_img[i].flatten(),NB_SELECTED_VAL, replace=False))\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mean = dict.fromkeys(dict_val.keys())\n",
    "dict_std = dict.fromkeys(dict_val.keys())\n",
    "\n",
    "for band in dict_val.keys():\n",
    "    dict_mean[band] = statistics.mean(dict_val[band])\n",
    "    dict_std[band] = statistics.stdev(dict_val[band])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1279.534254, 1016.734146, 925.27579, 793.929164, 1073.835362, 1909.174038, 2299.416608, 2270.341238, 739.972412, 14.35029, 1872.530084, 1055.580112, 2581.31964])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_mean.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([217.06847657849937, 236.49447129038893, 254.3062726895201, 383.2039520109347, 368.1552150776269, 508.3797488499248, 648.9503962237852, 672.6241209212196, 250.63210405653427, 9.4263874644246, 805.0923719290897, 632.9663115986274, 746.86130213707])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_std.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
